{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning based Human Pose Estimation using OpenCV\n",
    "- 출처 : https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/\n",
    "- 논문 : https://arxiv.org/pdf/1611.08050.pdf\n",
    "- We will explain in detail how to use a **pre-trained Caffe model** that won the **COCO keypoints challenge in 2016** in your own application\n",
    "![title](https://www.learnopencv.com/wp-content/uploads/2018/05/openpose-body-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation (a.k.a Keypoint Detection)\n",
    "- Pose Estimation is a general problem in Computer Vision where we **detect the position** and **orientation of an object.** This usually means ***detecting keypoint locations that describe the object.***\n",
    "- In this article, we will focus on human pose estimation, where it is required to detect and localize the major parts/joints of the body ( e.g. shoulders, ankle, knee, wrist etc. )\n",
    "\n",
    "### Data Set\n",
    "- COCO Keypoints challenge : http://cocodataset.org/#keypoints-2018 (18)\n",
    "- MPII Human Pose Dataset : http://human-pose.mpi-inf.mpg.de/ (15)\n",
    "- VGG Pose Dataset : http://www.robots.ox.ac.uk/~vgg/data/pose_evaluation/\n",
    "![title](https://www.learnopencv.com/wp-content/uploads/2018/05/coco-mpi-keypoints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Output Format\n",
    "Nose – 0, Neck – 1, Right Shoulder – 2, Right Elbow – 3, Right Wrist – 4,\n",
    "Left Shoulder – 5, Left Elbow – 6, Left Wrist – 7, Right Hip – 8,\n",
    "Right Knee – 9, Right Ankle – 10, Left Hip – 11, Left Knee – 12,\n",
    "LAnkle – 13, Right Eye – 14, Left Eye – 15, Right Ear – 16,\n",
    "Left Ear – 17, Background – 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Network\n",
    "We are using models trained on **Caffe Deep Learning Framework.** Caffe models have 2 files\n",
    "\n",
    "- prototxt file : which specifies the architecture of the neural network – how the different layers are arranged etc.\n",
    "- caffemodel file : which stores the weights of the trained model\n",
    "We will use these two files to load the network into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the paths for the 2 files\n",
    "import cv2\n",
    "protoFile = \"pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "weightsFile = \"pose_iter_160000.caffemodel\"\n",
    "\n",
    "# Read the network into Memory\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read Image and Prepare Input to the Network\n",
    "- The input frame that we read using OpenCV should be converted to a input blob ( like Caffe ) so that it can be fed to the network. This is done using the blobFromImage function which converts the image from OpenCV format to Caffe blob format. The parameters are to be provided in the blobFromImage function. First we normalize the pixel values to be in (0,1). Then we specify the dimensions of the image. Next, the Mean value to be subtracted, which is (0,0,0). There is no need to swap the R and B channels since both OpenCV and Caffe use BGR format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Parse Keypoints\n",
    "- Once the image is passed to the model, the predictions can be made using a single line of code. The forward method for the DNN class in OpenCV makes a forward pass through the network which is just another way of saying it is making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output is a 4D matrix\n",
    "\n",
    "    - The first dimension being the image ID ( in case you pass more than one image to the network ).\n",
    "    - The second dimension indicates the index of a keypoint. The model produces Confidence Maps and Part Affinity maps which are all concatenated. For COCO model it consists of 57 parts – 18 keypoint confidence Maps + 1 background + 19*2 Part Affinity Maps. Similarly, for MPI, it produces 44 points. We will be using only the first few points which correspond to Keypoints.\n",
    "    - The third dimension is the height of the output map.\n",
    "    - The fourth dimension is the width of the output map.\n",
    "\n",
    "- We check whether each keypoint is present in the image or not. We get the location of the keypoint by finding the maxima of the confidence map of that keypoint. We also use a threshold to reduce false detections.\n",
    "\n",
    "Once the keypoints are detected, we just plot them on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken in forward pass = 3.0552661418914795\n",
      "Keypoints - Nose : [(319, 64, 0.92282486)]\n",
      "Keypoints - Neck : [(300, 103, 0.7920166)]\n",
      "Keypoints - R-Sho : [(290, 94, 0.7809046)]\n",
      "Keypoints - R-Elb : [(251, 143, 0.7560005)]\n",
      "Keypoints - R-Wr : [(241, 201, 0.69001555)]\n",
      "Keypoints - L-Sho : [(310, 103, 0.69582427)]\n",
      "Keypoints - L-Elb : [(340, 153, 0.75905627)]\n",
      "Keypoints - L-Wr : [(387, 135, 0.6898169)]\n",
      "Keypoints - R-Hip : [(300, 203, 0.6197148)]\n",
      "Keypoints - R-Knee : [(340, 281, 0.7662977)]\n",
      "Keypoints - R-Ank : [(370, 370, 0.68540174)]\n",
      "Keypoints - L-Hip : [(299, 203, 0.46603516)]\n",
      "Keypoints - L-Knee : [(269, 281, 0.6894794)]\n",
      "Keypoints - L-Ank : [(221, 351, 0.7877196)]\n",
      "Keypoints - R-Eye : [(310, 63, 0.8117206)]\n",
      "Keypoints - L-Eye : [(319, 64, 0.25074452)]\n",
      "Keypoints - R-Ear : [(299, 64, 0.8688143)]\n",
      "Keypoints - L-Ear : []\n",
      "No Connection : k = 16\n",
      "No Connection : k = 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "image1 = cv2.imread(\"walk_1.jpg\")\n",
    "\n",
    "protoFile = \"pose_deploy_linevec.prototxt\"\n",
    "weightsFile = \"pose_iter_440000.caffemodel\"\n",
    "nPoints = 18\n",
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "              [2,17], [5,16] ]\n",
    "\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],\n",
    "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],\n",
    "          [47,48], [49,50], [53,54], [51,52], [55,56],\n",
    "          [37,38], [45,46]]\n",
    "\n",
    "colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\n",
    "\n",
    "\n",
    "def getKeypoints(probMap, threshold=0.1):\n",
    "\n",
    "    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)\n",
    "\n",
    "    mapMask = np.uint8(mapSmooth>threshold)\n",
    "    keypoints = []\n",
    "\n",
    "    #find the blobs\n",
    "    _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    #for each blob find the maxima\n",
    "    for cnt in contours:\n",
    "        blobMask = np.zeros(mapMask.shape)\n",
    "        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
    "        maskedProbMap = mapSmooth * blobMask\n",
    "        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
    "        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "# Find valid connections between the different joints of a all persons present\n",
    "def getValidPairs(output):\n",
    "    valid_pairs = []\n",
    "    invalid_pairs = []\n",
    "    n_interp_samples = 10\n",
    "    paf_score_th = 0.1\n",
    "    conf_th = 0.7\n",
    "    # loop for every POSE_PAIR\n",
    "    for k in range(len(mapIdx)):\n",
    "        # A->B constitute a limb\n",
    "        pafA = output[0, mapIdx[k][0], :, :]\n",
    "        pafB = output[0, mapIdx[k][1], :, :]\n",
    "        pafA = cv2.resize(pafA, (frameWidth, frameHeight))\n",
    "        pafB = cv2.resize(pafB, (frameWidth, frameHeight))\n",
    "\n",
    "        # Find the keypoints for the first and second limb\n",
    "        candA = detected_keypoints[POSE_PAIRS[k][0]]\n",
    "        candB = detected_keypoints[POSE_PAIRS[k][1]]\n",
    "        nA = len(candA)\n",
    "        nB = len(candB)\n",
    "\n",
    "        # If keypoints for the joint-pair is detected\n",
    "        # check every joint in candA with every joint in candB\n",
    "        # Calculate the distance vector between the two joints\n",
    "        # Find the PAF values at a set of interpolated points between the joints\n",
    "        # Use the above formula to compute a score to mark the connection valid\n",
    "\n",
    "        if( nA != 0 and nB != 0):\n",
    "            valid_pair = np.zeros((0,3))\n",
    "            for i in range(nA):\n",
    "                max_j=-1\n",
    "                maxScore = -1\n",
    "                found = 0\n",
    "                for j in range(nB):\n",
    "                    # Find d_ij\n",
    "                    d_ij = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                    norm = np.linalg.norm(d_ij)\n",
    "                    if norm:\n",
    "                        d_ij = d_ij / norm\n",
    "                    else:\n",
    "                        continue\n",
    "                    # Find p(u)\n",
    "                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\n",
    "                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\n",
    "                    # Find L(p(u))\n",
    "                    paf_interp = []\n",
    "                    for k in range(len(interp_coord)):\n",
    "                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\n",
    "                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ])\n",
    "                    # Find E\n",
    "                    paf_scores = np.dot(paf_interp, d_ij)\n",
    "                    avg_paf_score = sum(paf_scores)/len(paf_scores)\n",
    "\n",
    "                    # Check if the connection is valid\n",
    "                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair\n",
    "                    if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :\n",
    "                        if avg_paf_score > maxScore:\n",
    "                            max_j = j\n",
    "                            maxScore = avg_paf_score\n",
    "                            found = 1\n",
    "                # Append the connection to the list\n",
    "                if found:\n",
    "                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)\n",
    "\n",
    "            # Append the detected connections to the global list\n",
    "            valid_pairs.append(valid_pair)\n",
    "        else: # If no keypoints are detected\n",
    "            print(\"No Connection : k = {}\".format(k))\n",
    "            invalid_pairs.append(k)\n",
    "            valid_pairs.append([])\n",
    "    return valid_pairs, invalid_pairs\n",
    "\n",
    "\n",
    "\n",
    "# This function creates a list of keypoints belonging to each person\n",
    "# For each detected valid pair, it assigns the joint(s) to a person\n",
    "def getPersonwiseKeypoints(valid_pairs, invalid_pairs):\n",
    "    # the last number in each row is the overall score\n",
    "    personwiseKeypoints = -1 * np.ones((0, 19))\n",
    "\n",
    "    for k in range(len(mapIdx)):\n",
    "        if k not in invalid_pairs:\n",
    "            partAs = valid_pairs[k][:,0]\n",
    "            partBs = valid_pairs[k][:,1]\n",
    "            indexA, indexB = np.array(POSE_PAIRS[k])\n",
    "\n",
    "            for i in range(len(valid_pairs[k])):\n",
    "                found = 0\n",
    "                person_idx = -1\n",
    "                for j in range(len(personwiseKeypoints)):\n",
    "                    if personwiseKeypoints[j][indexA] == partAs[i]:\n",
    "                        person_idx = j\n",
    "                        found = 1\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    personwiseKeypoints[person_idx][indexB] = partBs[i]\n",
    "                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]\n",
    "\n",
    "                # if find no partA in the subset, create a new subset\n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(19)\n",
    "                    row[indexA] = partAs[i]\n",
    "                    row[indexB] = partBs[i]\n",
    "                    # add the keypoint_scores for the two keypoints and the paf_score\n",
    "                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]\n",
    "                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])\n",
    "    return personwiseKeypoints\n",
    "\n",
    "\n",
    "frameWidth = image1.shape[1]\n",
    "frameHeight = image1.shape[0]\n",
    "\n",
    "t = time.time()\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "# Fix the input Height and get the width according to the Aspect Ratio\n",
    "inHeight = 368 \n",
    "inWidth = int((inHeight/frameHeight)*frameWidth)\n",
    "\n",
    "# Prepare the frame to be fed to the network\n",
    "inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n",
    "                          (0, 0, 0), swapRB=False, crop=False)\n",
    "# Set the prepared object as the input blob of the network\n",
    "net.setInput(inpBlob)\n",
    "output = net.forward()\n",
    "print(\"Time Taken in forward pass = {}\".format(time.time() - t))\n",
    "\n",
    "detected_keypoints = []\n",
    "keypoints_list = np.zeros((0,3))\n",
    "keypoint_id = 0\n",
    "threshold = 0.1\n",
    "\n",
    "for part in range(nPoints):\n",
    "    # confidence map of corresponding body's part.\n",
    "    probMap = output[0,part,:,:]\n",
    "    probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n",
    "    keypoints = getKeypoints(probMap, threshold)\n",
    "    print(\"Keypoints - {} : {}\".format(keypointsMapping[part], keypoints))\n",
    "    keypoints_with_id = []\n",
    "    for i in range(len(keypoints)):\n",
    "        keypoints_with_id.append(keypoints[i] + (keypoint_id,))\n",
    "        keypoints_list = np.vstack([keypoints_list, keypoints[i]])\n",
    "        keypoint_id += 1\n",
    "\n",
    "    detected_keypoints.append(keypoints_with_id)\n",
    "\n",
    "\n",
    "frameClone = image1.copy()\n",
    "for i in range(nPoints):\n",
    "    for j in range(len(detected_keypoints[i])):\n",
    "        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 5, colors[i], -1, cv2.LINE_AA)\n",
    "cv2.imshow(\"Keypoints\",frameClone)\n",
    "\n",
    "valid_pairs, invalid_pairs = getValidPairs(output)\n",
    "personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)\n",
    "\n",
    "for i in range(17):\n",
    "    for n in range(len(personwiseKeypoints)):\n",
    "        index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]\n",
    "        if -1 in index:\n",
    "            continue\n",
    "        B = np.int32(keypoints_list[index.astype(int), 0])\n",
    "        A = np.int32(keypoints_list[index.astype(int), 1])\n",
    "        cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Detected Pose\" , frameClone)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
